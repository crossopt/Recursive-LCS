# -*- coding: utf-8 -*-
"""Untitled4.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1XzI1u0OPMVT4JYTzCyI8hz11dIRSZ3ec
"""

import requests
import re
import warnings
import nltk
import string

from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer
from nltk.corpus import stopwords

from gensim import corpora, models

import matplotlib.pyplot as plt

nltk.download('punkt')
nltk.download('wordnet')
nltk.download('stopwords')

def get_data(url):
    return requests.get(url).text

oldls = ['https://www.gutenberg.org/cache/epub/48138/pg48138.txt',
         'https://www.gutenberg.org/files/145/145-0.txt',
         'https://www.gutenberg.org/files/48137/48137-0.txt',
         'https://www.gutenberg.org/cache/epub/10799/pg10799.txt', 
         'https://www.gutenberg.org/cache/epub/11889/pg11889.txt',
         'https://www.gutenberg.org/cache/epub/12398/pg12398.txt',
         'https://www.gutenberg.org/cache/epub/9798/pg9798.txt',
         'https://www.gutenberg.org/cache/epub/11364/pg11364.txt',
         'https://www.gutenberg.org/cache/epub/10462/pg10462.txt',
         'https://www.gutenberg.org/cache/epub/12180/pg12180.txt',
         'https://www.gutenberg.org/files/1342/1342-0.txt',
         'https://www.gutenberg.org/files/141/141-0.txt',
         'https://www.gutenberg.org/files/158/158-0.txt',
         'https://www.gutenberg.org/files/145/145-0.txt']

links = []
for i in range(0, 100):
    links.append('https://www.gutenberg.org/cache/epub/{}/pg{}.txt'.format(10100 + i, 10100 + i))

mm = ' '.join([get_data(link) for link in links])

def is_word(seq):
    return seq.isalpha()

def get_words(text):
    words = list(filter(is_word, word_tokenize(text)))
    return [word.lower() for word in words]

def lemmatize(tokenized_text):
    lemmatizer = WordNetLemmatizer()
    return [lemmatizer.lemmatize(word) for word in tokenized_text]

def remove_stopwords(lemmatized_text):
    stop = set(stopwords.words('english'))
    return [word for word in lemmatized_text if word not in stop]

def clean_text(text):
    return remove_stopwords(lemmatize(get_words(text)))

clean_mm = clean_text(mm)[1:]

fsave = ' '.join(clean_mm)
fout = open("mm_clean", "w")
print(fsave, file=fout)
fout.close()

